{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3d376d",
   "metadata": {},
   "source": [
    "# Logistic regression algorithm written in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8116fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d00b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Applies the sigmoid function to every element in input matrix/vector. \n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: z - numpy.ndarray of shape (N,)\n",
    "    Output: g(z) - np.ndarray of shape (N,) - sigmoid function applied to each element of z\n",
    "    '''\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost(X, y, W, b, lambda_):\n",
    "    '''\n",
    "    Calculates the cost function for a model applied to input features. The lower the cost, the better the model fits.\n",
    "    The cost function can punish large parameter values for the model if reparameterisation (lambda_ != 0) is included.\n",
    "    To get the true cost function of the data, do not include reparameterisation (lambda_ == 0).\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X - pandas.core.frame.DataFrame shape (M,N) - features of dataset with M indices and N features\n",
    "           y - pandas.core.frame.DataFrame shape (M,1) - target class of dataset with M indices\n",
    "           W - numpy.ndarray of shape (N,) - feature parameters for model\n",
    "           b - float - offset parameter for model\n",
    "           lambda_ - float - reparameterisation parameter\n",
    "    Output: cost - float - cost function of model with parameter W and b fitted to true data y with features X\n",
    "    '''\n",
    "    M = len(X.index)\n",
    "    f_wb_vec = sigmoid(np.matmul(X.to_numpy(),W) + b)\n",
    "    \n",
    "    #Overflow issues with upcoming log's if entries are too close to zero or one\n",
    "    f_wb_vec[f_wb_vec<1e-15]=1e-15\n",
    "    f_wb_vec[1-f_wb_vec<1e-15]=1-1e-15\n",
    "    \n",
    "    loss_vec = -y.to_numpy().reshape(M,1)*np.log(f_wb_vec) - (1-y.to_numpy().reshape(M,1))*np.log(1 - f_wb_vec)\n",
    "\n",
    "    \n",
    "    cost = (sum(loss_vec)/M + (lambda_/(2*M))*np.sum(W*W))[0]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d9f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, W, b, lambda_):\n",
    "    '''\n",
    "    Computes the partial derivatives of the cost function with respect to model parameters.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X - pandas.core.frame.DataFrame shape (M,N) - features of dataset with M indices and N features\n",
    "           y - pandas.core.frame.DataFrame shape (M,1) - target class of dataset with M indices\n",
    "           W - numpy.ndarray of shape (N,1) - feature parameters for model\n",
    "           b - float - offset parameter for model\n",
    "           lambda_ - float - reparameterisation parameter\n",
    "    Output:dj_db - float - partial derivative of cost function with respect to model parameter b\n",
    "           dj_dW - numpy.ndarray of shape (N,1) - partial derivative of cost function wwith respect to model parameters in vector W\n",
    "    '''\n",
    "    (M,N) = X.shape\n",
    "    dj_dW = np.array(np.zeros(N)).reshape(N,1)\n",
    "    dj_db = 0.0\n",
    "    \n",
    "    f_wb_vec = sigmoid(np.matmul(X.to_numpy(),W) + b)\n",
    "    dj_dW_vec = X.to_numpy()*(f_wb_vec - y.to_numpy().reshape(M,1))\n",
    "    dj_db_vec = (f_wb_vec - y.to_numpy().reshape(M,1))\n",
    "    dj_dW = np.sum(dj_dW_vec,axis=0).reshape(N,1)/M + lambda_*W/M\n",
    "    dj_db = np.sum(dj_db_vec,axis=0)/M\n",
    "    \n",
    "    return dj_db, dj_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8f25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W_init, b_init, lambda_, alpha=1, max_iter=100, print_step=20, save_step=10):\n",
    "    '''\n",
    "    Perform gradient descent algorithm to minimise cost function by changing the model parameters.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X - pandas.core.frame.DataFrame shape (M,N) - features of dataset with M indices and N features\n",
    "           y - pandas.core.frame.DataFrame shape (M,1) - target class of dataset with M indices\n",
    "           W_init - numpy.ndarray shape (N,1) - initial feature parameters for model\n",
    "           b_init - float - initial offset parameter for model\n",
    "           alpha - float - learning rate of gradient descent\n",
    "           max_iter - int - maximum number of iterations for gradient descent\n",
    "           print_step - int - on iterations that are a multiple of print_step, print percentage of completion. If False do not print progress\n",
    "           save_step - int - on iterations that are a multiple of save_step, store cost, models parameters and iterations in to vectors. If False only save last iteration\n",
    "           lambda_ - float - reparameterisation parameter\n",
    "    Output:iter_history - numpy.array - saved iterations for each multiple of save_step in max_iter \n",
    "           cost_reg_history - numpy.ndarray - cost function at saved iterations\n",
    "           W - numpy.ndarray shape (N,1) - final model parameters\n",
    "           b - float - final offset model parameter\n",
    "           final_cost - float - final cost function of dataset (true cost does not use reparameterisation term) \n",
    "    '''\n",
    "    \n",
    "    #Initialisations\n",
    "    cost_reg_history, iter_history, final_cost = [], [], 0.\n",
    "    W, b = W_init, b_init\n",
    "    for iter in range(1,max_iter+1):\n",
    "        dj_db, dj_dW = compute_gradient(X, y, W, b, lambda_)   \n",
    "        \n",
    "        W = W - alpha*dj_dW\n",
    "        b = b - alpha*dj_db\n",
    "        \n",
    "        \n",
    "        cost_reg = calc_cost(X, y, W, b, lambda_)\n",
    "        \n",
    "        if(save_step!=0):\n",
    "            if(iter % save_step or save_step==1):\n",
    "                cost_reg_history.append(cost_reg)\n",
    "                iter_history.append(iter) \n",
    "        elif(iter==max_iter):\n",
    "            cost_reg_history.append(cost_reg)\n",
    "            iter_history.append(iter) \n",
    "                \n",
    "        if(print_step):\n",
    "            if(iter % print_step == 0):print(\"{}%\\n\".format(100*iter/max_iter))\n",
    "        \n",
    "    #Calculate final cost of the model - note that this is the true cost\n",
    "    #and so does not include reparameterisation (set lambda_=0.)    \n",
    "    final_cost = calc_cost(X, y, W, b, lambda_=0.)\n",
    "        \n",
    "    return cost_reg_history, W, b, iter_history, final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f6e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X):\n",
    "    '''\n",
    "    Applies min max scaling to input features.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X - pandas.core.frame.DataFrame - features of shape (M,N) with M indices and N features\n",
    "    Output: X_minmax - pandas.core.frame.DataFrame - minmax scaled features of shape (M,N) with M indices and N features\n",
    "    '''\n",
    "    N = len(X.columns)\n",
    "    X_minmax = X.copy(deep=True)\n",
    "    for n in range(N):\n",
    "        X_minmax.iloc[:,n] =  (X.iloc[:,n] - min(X.iloc[:,n]))/(max(X.iloc[:,n]) - min(X.iloc[:,n]))\n",
    "    return X_minmax\n",
    "\n",
    "def encode_polynomial(X, degree=1):\n",
    "    '''\n",
    "    Calculate polynomial features from input features.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X - pandas.core.frame.DataFrame - features of shape (M,N) with M indices and N features\n",
    "           degree - float - maximum exponent applied to each feature\n",
    "    Output: X_poly - pandas.core.frame.DataFrame - polynomial features of shape (M,N*degree) with M indices and N*degree features\n",
    "    '''\n",
    "    N = len(X.columns)\n",
    "    N_poly = N*degree\n",
    "    \n",
    "    X_poly = X.copy(deep=True)\n",
    "\n",
    "    for n in range(0,N):\n",
    "        for d in range(1,degree+1):\n",
    "            feat_label = \"{}^{},\".format(X.columns[n],d)\n",
    "            #X_poly[feat_label] = X.iloc[:,n]**d\n",
    "            \n",
    "            new_feat = X.iloc[:,n].copy(deep=True)\n",
    "            new_feat = new_feat**degree\n",
    "            new_feat.rename(index=feat_label, inplace=True)\n",
    "            X_poly = pd.concat([X_poly, new_feat], axis=1)\n",
    "    X_poly.drop(labels=X.columns,axis=1,inplace=True)    \n",
    "    \n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de41e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, X_test, y_train, y_test, lambda_, alpha=1, f_cutoff=0.5, degree=1, max_iter=100, print_step=20):\n",
    "    '''\n",
    "    Takes in input training data, uses gradient descent to find an optimal model, applies model to test features.\n",
    "    After applying model to test features, estimated target class data is obtained which is compared to test target class data.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Input: X_train - pandas.core.frame.DataFrame - training set features of shape (M_train,N) with M_train indices and N features\n",
    "           y_train - pandas.core.frame.DataFrame - training set of shape (M_train,1) with target feature for each index\n",
    "           X_test - pandas.core.frame.DataFrame - training set features of shape (M_test,N) with M_test indices and N features\n",
    "           y_test - pandas.core.frame.DataFrame - training set of shape (M_test,1) with target feature for each index\n",
    "           lambda_ - float - reparameterisation parameter\n",
    "           alpha - float - learning rate of gradient descent\n",
    "           f_cutoff - float - cutoff for sigmoid function to determine if y_hat is equal to 0 or 1.\n",
    "           degree - int - powers of input features\n",
    "           max_iter - int - maximum number of iterations for gradient descent\n",
    "           print_step - int - on iterations that are a multiple of print_step, print percentage of completion. If False do not print progress\n",
    "    Output:y_hat - numpy.array - predicted target features of shape (M_train,1) \n",
    "           cost_reg_history - numpy.ndarray - cost function at saved iterations\n",
    "           W_model - numpy.ndarray shape (N,1) - final model parameters\n",
    "           b_history - float - final offset model parameter\n",
    "           final_cost - float - final cost function of dataset (true cost does not use reparameterisation term) \n",
    "    '''\n",
    "    X_train = encode_polynomial(X_train,degree)\n",
    "    X_train = min_max_scale(X_train)\n",
    "    \n",
    "    X_test = encode_polynomial(X_test,degree)\n",
    "    X_test = min_max_scale(X_test)\n",
    "    \n",
    "    N_train = len(X_train.columns)\n",
    "    W = np.ones(N_train).reshape(N_train,1)\n",
    "    b = 0.\n",
    "    cost_reg_history, W_model, b_model, print_steps, train_cost = gradient_descent(X_train, y_train, W, b, lambda_, alpha, max_iter, print_step, save_step=0)\n",
    "\n",
    "    M_test = len(X_test.index)\n",
    "    \n",
    "    f_wb_vec = sigmoid(np.matmul(X_test.to_numpy(),W_model) + b_model)\n",
    "    f_wb_vec[f_wb_vec >= f_cutoff] = 1\n",
    "    f_wb_vec[f_wb_vec < f_cutoff] = 0\n",
    "    \n",
    "    y_hat = y_test.copy(deep=True)\n",
    "    y_hat.iloc[:] = np.squeeze(f_wb_vec.reshape(1,M_test))\n",
    "    \n",
    "    test_cost = calc_cost(X_test, y_test, W_model, b_model, lambda_=0.)\n",
    "                       \n",
    "    return y_hat, W_model, b_model, train_cost, test_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
